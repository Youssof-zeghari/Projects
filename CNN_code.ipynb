{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Pj-vyFZ80sQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbbf1684-1755-4b9e-8c3b-e8818443f317"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.datasets import mnist\n",
        "\n",
        "# Load MNIST dataset\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Normalize the data\n",
        "X_train, X_test = X_train / 255.0, X_test / 255.0\n",
        "\n",
        "# Hyperparameters\n",
        "input_size = 28\n",
        "num_classes = 10\n",
        "filter_size = 3\n",
        "num_filters_1 = 8\n",
        "num_filters_2 = 16\n",
        "pool_size = 2\n",
        "learning_rate = 0.001\n",
        "epochs = 10\n",
        "\n",
        "# Initialize filters and weights\n",
        "def initialize_filters():\n",
        "    filters_1 = np.random.randn(num_filters_1, 1, filter_size, filter_size) * 0.1\n",
        "    filters_2 = np.random.randn(num_filters_2, num_filters_1, filter_size, filter_size) * 0.1\n",
        "    flatten_size = num_filters_2 * 5 * 5  # 16 * 5 * 5 = 400\n",
        "    weights_fc = np.random.randn(flatten_size, num_classes) * 0.1\n",
        "    biases_fc = np.zeros(num_classes)\n",
        "\n",
        "    return filters_1, filters_2, weights_fc, biases_fc\n",
        "\n",
        "filters_1, filters_2, weights_fc, biases_fc = initialize_filters()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def convolution(input, filters):\n",
        "    num_filters, input_depth, filter_size, _ = filters.shape\n",
        "    output_dim = input.shape[1] - filter_size + 1\n",
        "    output = np.zeros((num_filters, output_dim, output_dim))\n",
        "\n",
        "    for f in range(num_filters):\n",
        "        for i in range(output_dim):\n",
        "            for j in range(output_dim):\n",
        "                region = input[:, i:i+filter_size, j:j+filter_size]\n",
        "                output[f, i, j] = np.sum(region * filters[f])\n",
        "\n",
        "    return output"
      ],
      "metadata": {
        "id": "GCzJPkKK9Kui"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def relu(feature_map):\n",
        "    return np.maximum(0, feature_map)"
      ],
      "metadata": {
        "id": "4uI_UgIC-Qgi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def max_pooling(input, size=2, stride=2):\n",
        "    num_filters, height, width = input.shape\n",
        "    output_dim = (height - size) // stride + 1\n",
        "    pooled_output = np.zeros((num_filters, output_dim, output_dim))\n",
        "\n",
        "    for f in range(num_filters):\n",
        "        for i in range(0, height - size + 1, stride):\n",
        "            for j in range(0, width - size + 1, stride):\n",
        "                pooled_output[f, i // stride, j // stride] = np.max(input[f, i:i+size, j:j+size])\n",
        "\n",
        "    return pooled_output"
      ],
      "metadata": {
        "id": "Vyq9vve9-Stc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def flatten(input):\n",
        "    return input.reshape(-1)"
      ],
      "metadata": {
        "id": "GgiSCU1Q-UlM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fully_connected(input, weights, biases):\n",
        "    return np.dot(input, weights) + biases\n",
        "\n",
        "def softmax(x):\n",
        "    exp_x = np.exp(x - np.max(x))\n",
        "    return exp_x / np.sum(exp_x)"
      ],
      "metadata": {
        "id": "qctJM3nt-Waf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cross_entropy_loss(y_true, y_pred):\n",
        "    return -np.sum(y_true * np.log(y_pred + 1e-9))"
      ],
      "metadata": {
        "id": "CXJbkgAv-YZF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def forward_propagation(image):\n",
        "    # Reshape the image to include a channel dimension (1, 28, 28)\n",
        "    image = image.reshape(1, input_size, input_size)\n",
        "\n",
        "    # First Convolution + ReLU + Max Pooling\n",
        "    conv1 = convolution(image, filters_1)\n",
        "    relu1 = relu(conv1)\n",
        "    pool1 = max_pooling(relu1)\n",
        "\n",
        "    # Second Convolution + ReLU + Max Pooling\n",
        "    conv2 = convolution(pool1, filters_2)\n",
        "    relu2 = relu(conv2)\n",
        "    pool2 = max_pooling(relu2)\n",
        "\n",
        "    # Flatten and Fully Connected Layer\n",
        "    flat = flatten(pool2)  # This should give a size of 400\n",
        "    #print(f\"Flatten shape: {flat.shape}\")  # Should print (400,)\n",
        "    logits = fully_connected(flat, weights_fc, biases_fc)\n",
        "    output = softmax(logits)\n",
        "\n",
        "    return output, conv1, pool1, conv2, pool2, flat"
      ],
      "metadata": {
        "id": "endpg8Z6-af_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def update_parameters(output, y_true):\n",
        "    global weights_fc, biases_fc\n",
        "    gradient = output - y_true\n",
        "    weights_fc -= learning_rate * np.outer(flat, gradient)\n",
        "    biases_fc -= learning_rate * gradient"
      ],
      "metadata": {
        "id": "QIpgv6w5-dAD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(epochs):\n",
        "    correct_predictions = 0\n",
        "    total_loss = 0\n",
        "\n",
        "    for i in range(len(X_train)):\n",
        "        image = X_train[i]\n",
        "        label = np.zeros(num_classes)\n",
        "        label[y_train[i]] = 1\n",
        "        # Forward pass\n",
        "        output, conv1, pool1, conv2, pool2, flat = forward_propagation(image)\n",
        "\n",
        "        # Calculate loss and update parameters\n",
        "        loss = cross_entropy_loss(label, output)\n",
        "        total_loss += loss\n",
        "        update_parameters(output, label)\n",
        "\n",
        "        # Check accuracy\n",
        "        if np.argmax(output) == y_train[i]:\n",
        "            correct_predictions += 1\n",
        "\n",
        "    accuracy = correct_predictions / len(X_train)\n",
        "    print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}, Accuracy: {accuracy:.4f}\")"
      ],
      "metadata": {
        "id": "Pey0QfjA-iro"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "correct_predictions = 0\n",
        "\n",
        "for i in range(len(X_test)):\n",
        "    image = X_test[i]\n",
        "    label = y_test[i]\n",
        "\n",
        "    output, _, _, _, _, _ = forward_propagation(image)\n",
        "\n",
        "    if np.argmax(output) == label:\n",
        "        correct_predictions += 1\n",
        "\n",
        "test_accuracy = correct_predictions / len(X_test)\n",
        "print(f\"Test Accuracy: {test_accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BttRjjFj-ltc",
        "outputId": "4ff96163-6722-4fa0-9a20-e78e0bf9898a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.8589\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BqmA9yDbdI4B"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}